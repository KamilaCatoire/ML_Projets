-----------------------------------------------------------------------------------------------------------------------------

*This project is a part of my Jedha Data Fullstack program certification*

-----------------------------------------------------------------------------------------------------------------------------

# JEDHA PROJECT INFORMATION. SUPERVISED ML

## Conversion challenge


Engage in a machine learning competition with classmates, similar to Kaggle competitions, where the performance of participants is assessed independently by a teacher and displayed on a leaderboard.

### Context:
The competition involves working with imbalanced data obtained from a website that publishes a newsletter. The objective is to develop a model that can predict whether a particular user will subscribe to the newsletter, despite having limited available information. Additionally, the aim is to analyze the model's parameters in order to identify the most significant features that explain user behavior.

The process consists of four steps:

    -- Conduct exploratory data analysis (EDA), apply all necessary data preprocessing techniques, and train a baseline model.
    -- Enhance the model's f1-score on the test set through techniques such as feature engineering, regularization, non-linear models, hyperparameter optimization, among others.
    -- Once satisfied with the model's performance, utilize it to generate predictions using unlabeled test data. The predictions should be saved in a .csv file that will be submitted to the teacher. Participants are allowed to make multiple submissions and are encouraged to experiment with different models.
    
### Competition
On the day of the competition in the class, I secured the seventh position with a f1-score of 0.7552902875 using the Logistic Regression model, while the top score in the class was 0.757396. I consider my performance on that day as my baseline model and now I aim to improve my score by trying out various models such as Decision Tree, Random Forest, and Random Forest with GridSearch method.

The best score is 0.984568 using Random Forest with Gridsearch and SMOTE method.

*** SMOTE (Synthetic Minority Over-sampling Technique) is a method used to address the class imbalance problem in machine learning. Class imbalance occurs when the number of samples in one class is significantly smaller than the number of samples in the other class. ***

